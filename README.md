# -Sign-Language-Recognition-System-
This project is a real-time sign language recognition system designed to help bridge communication gaps between deaf individuals and hearing users. Combining computer vision, deep learning, and NLP, it translates both static and dynamic hand gestures into text and speech, offering an accessible, desktop-based solution for inclusive communication.
ğŸš€ Features:
ğŸ”¤ Recognizes single-hand static gestures (Aâ€“Z, 0â€“9) using CNN

âœ‹ Detects two-handed static words with MLP models

ğŸ¬ Identifies dynamic gestures with LSTM networks

ğŸ“· Real-time gesture detection via webcam (OpenCV)

ğŸ–ï¸ Hand & pose landmarks extraction with MediaPipe for high accuracy

ğŸ’¡ Next-word prediction using NLP bigram models for faster typing

ğŸ—£ï¸ Auto speech output with voice selection & adjustable speech rate (Pyttsx3)

ğŸ’» Tkinter-based GUI enabling seamless bidirectional communication

âœ… Achieves over 90% accuracy in testing scenarios

ğŸ›  Tech Stack:
Python, OpenCV, MediaPipe

TensorFlow, NumPy, scikit-learn

Pyttsx3 (text-to-speech)

Tkinter (GUI)

NLP bigram models

ğŸ“š This project demonstrates how AI and deep learning can make technology more inclusive and accessible for the deaf community.
