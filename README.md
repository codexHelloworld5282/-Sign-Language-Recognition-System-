# -Sign-Language-Recognition-System-
This project is a real-time sign language recognition system designed to help bridge communication gaps between deaf individuals and hearing users. Combining computer vision, deep learning, and NLP, it translates both static and dynamic hand gestures into text and speech, offering an accessible, desktop-based solution for inclusive communication.
🚀 Features:
🔤 Recognizes single-hand static gestures (A–Z, 0–9) using CNN

✋ Detects two-handed static words with MLP models

🎬 Identifies dynamic gestures with LSTM networks

📷 Real-time gesture detection via webcam (OpenCV)

🖐️ Hand & pose landmarks extraction with MediaPipe for high accuracy

💡 Next-word prediction using NLP bigram models for faster typing

🗣️ Auto speech output with voice selection & adjustable speech rate (Pyttsx3)

💻 Tkinter-based GUI enabling seamless bidirectional communication

✅ Achieves over 90% accuracy in testing scenarios

🛠 Tech Stack:
Python, OpenCV, MediaPipe

TensorFlow, NumPy, scikit-learn

Pyttsx3 (text-to-speech)

Tkinter (GUI)

NLP bigram models

📚 This project demonstrates how AI and deep learning can make technology more inclusive and accessible for the deaf community.
