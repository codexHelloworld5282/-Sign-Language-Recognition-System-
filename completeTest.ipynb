{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3fd1045-e34d-4903-a957-c138ea7c06bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------BUTTONS SETUP CODE FOR START GUI FUNC----------\n",
    "\n",
    "def setup_buttons():\n",
    "    frame = tk.Frame(root, bg=\"#f0f0f0\")\n",
    "    frame.pack(pady=15)\n",
    "\n",
    "    # First row: white background with black text\n",
    "    btn_params_white = dict(\n",
    "    font=('Arial', 14),\n",
    "    bg='white',\n",
    "    fg='black',\n",
    "    activebackground='white',\n",
    "    activeforeground='black',\n",
    "    relief='raised',\n",
    "    padx=10, pady=5\n",
    "    )\n",
    "\n",
    "    tk.Button(frame, text=\"🗣️ Speak\", command=speak_text, **btn_params_white).grid(row=0, column=0, padx=10)\n",
    "    tk.Button(frame, text=\"🧹 Clear\", command=clear_text, **btn_params_white).grid(row=0, column=1, padx=10)\n",
    "    tk.Button(frame, text=\"🎧 Listen\", command=open_listen, **btn_params_white).grid(row=0, column=2, padx=10)\n",
    "    tk.Button(frame, text=\"🎙️ Voice Settings\", command=select_exact_voice_and_start_gui, **btn_params_white).grid(row=0, column=3, padx=10)\n",
    "\n",
    "    # Second row: black with white text\n",
    "    switch = tk.Frame(root, bg=\"#f0f0f0\")\n",
    "    switch.pack(pady=15)\n",
    "\n",
    "    btn_params_black = dict(\n",
    "        font=('Arial', 14),\n",
    "        bg='black',\n",
    "        fg='white',\n",
    "        activebackground='black',\n",
    "        activeforeground='white',\n",
    "        relief='raised',\n",
    "        padx=8, pady=5\n",
    "    )\n",
    "\n",
    "    tk.Button(switch, text=\"🔤 Alphabets\", command=load_alphabet_model, **btn_params_black).grid(row=0, column=0, padx=8, pady=5)\n",
    "    tk.Button(switch, text=\"🔢 Numbers\", command=load_number_model, **btn_params_black).grid(row=0, column=1, padx=8, pady=5)\n",
    "    tk.Button(switch, text=\"📝 Words\", command=load_word_model, **btn_params_black).grid(row=0, column=2, padx=8, pady=5)\n",
    "    tk.Button(switch, text=\"✋✋ Both Hands\", command=load_both_hand_model, **btn_params_black).grid(row=0, column=3, padx=8, pady=5)\n",
    "    tk.Button(switch, text=\"⚡ Dynamic\", command=load_dynamic_model, **btn_params_black).grid(row=0, column=4,padx=8, pady=5)\n",
    "        # Logout button: red base with black text\n",
    "    btn_params_red = dict(\n",
    "        font=('Arial', 14),\n",
    "        bg='#e74c3c',              # nice red\n",
    "        fg='black',\n",
    "        activebackground='#c0392b',  # darker red on hover\n",
    "        activeforeground='black',\n",
    "        relief='raised',\n",
    "        padx=8, pady=5\n",
    "    )\n",
    "\n",
    "    tk.Button(switch, text=\"🛑 Logout\", command=root.destroy, **btn_params_red).grid(row=0, column=5, padx=8, pady=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1cf4ac-dd90-4383-bc46-8263256c4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tkinter as tk\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# 🌱 Custom suggestion words list\n",
    "suggestion_words = [\n",
    "    \"hello\", \"my\", \"name\", \"is\", \"study\", \"in\", \"department\", \"ok\", \"am\", \"help\", \"good\", \"night\", \"morning\",\n",
    "    \"pray\", \"want\", \"see\", \"later\", \"when\", \"water\", \"you\", \"done\", \"i\", \"love\", \"u\", \"yes\", \"thanks\",\n",
    "    \"sign\", \"language\", \"deaf\", \"project\", \"fyp\", \"university\", \"student\", \"presentation\", \"today\",\n",
    "    \"introduce\", \"learn\", \"communicate\", \"understand\", \"gesture\", \"recognize\", \"recognition\",\n",
    "    \"speech\", \"text\", \"voice\", \"convert\", \"system\", \"using\", \"ai\", \"model\", \"deep\", \"learning\", \"thank\", \"team\", \"group\"\n",
    "]\n",
    "\n",
    "# ✏️ Simple bigram dictionary\n",
    "bigram_dict = {\n",
    "    \"hello\": [\"my\", \"i\", \"good\"],\n",
    "    \"my\": [\"name\", \"university\", \"project\"],\n",
    "    \"name\": [\"is\", \"in\", \"project\"],\n",
    "    \"i\": [\"am\", \"love\", \"want\"],\n",
    "    \"am\": [\"student\", \"learning\", \"good\"],\n",
    "    \"want\": [\"to\", \"help\", \"see\"],\n",
    "    \"to\": [\"learn\", \"study\", \"see\"],\n",
    "    \"study\": [\"in\", \"ai\", \"project\"],\n",
    "    \"in\": [\"university\", \"department\", \"project\"],\n",
    "    \"project\": [\"fyp\", \"presentation\", \"team\"],\n",
    "    \"sign\": [\"language\", \"system\", \"recognition\"],\n",
    "    \"language\": [\"model\", \"recognition\", \"system\"],\n",
    "    \"deaf\": [\"community\", \"culture\", \"student\"],\n",
    "    \"recognition\": [\"system\", \"model\", \"project\"],\n",
    "    \"good\": [\"morning\", \"night\", \"afternoon\"],\n",
    "    \"thank\": [\"you\", \"team\", \"group\"],\n",
    "    \"thanks\": [\"team\", \"you\", \"group\"],\n",
    "    \"team\": [\"project\", \"presentation\", \"group\"],\n",
    "    \"presentation\": [\"today\", \"team\", \"group\"],\n",
    "    \"learn\": [\"sign\", \"language\", \"ai\"],\n",
    "    \"help\": [\"me\", \"you\", \"project\"],\n",
    "    \"today\": [\"presentation\", \"project\", \"team\"],\n",
    "    \"fyp\": [\"project\", \"presentation\", \"team\"],\n",
    "    \"student\": [\"project\", \"presentation\", \"team\"],\n",
    "    \"using\": [\"ai\", \"model\", \"system\"],\n",
    "    \"ai\": [\"model\", \"system\", \"project\"],\n",
    "    \"model\": [\"recognition\", \"system\", \"project\"],\n",
    "    \"system\": [\"recognition\", \"model\", \"language\"],\n",
    "    \"speech\": [\"to\", \"text\", \"recognition\"],\n",
    "    \"text\": [\"to\", \"speech\", \"convert\"],\n",
    "    \"voice\": [\"recognition\", \"system\", \"text\"],\n",
    "    \"convert\": [\"speech\", \"text\", \"system\"],\n",
    "    \"love\": [\"you\", \"my\", \"sign\"],\n",
    "    \"you\": [\"are\", \"want\", \"help\"],\n",
    "    \"see\": [\"you\", \"project\", \"presentation\"],\n",
    "    \"communicate\": [\"with\", \"using\", \"sign\"],\n",
    "    \"understand\": [\"sign\", \"language\", \"project\"],\n",
    "    \"gesture\": [\"recognition\", \"system\", \"project\"],\n",
    "    \"recognize\": [\"sign\", \"gesture\", \"language\"],\n",
    "    \"introduce\": [\"my\", \"team\", \"project\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Globals\n",
    "current_suggestions = []\n",
    "suggestion_labels = []\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def create_suggestions_panel(parent):\n",
    "    panel = tk.Frame(parent, bg='white', padx=15, pady=15, relief='ridge', bd=2)\n",
    "\n",
    "    heading = tk.Label(\n",
    "        panel, text=\"✨ Next Possible Words\",\n",
    "        font=('Helvetica', 16, 'bold'),\n",
    "        fg='#00796B', bg='white'\n",
    "    )\n",
    "    heading.pack(anchor='w', pady=(0, 10))\n",
    "\n",
    "    global suggestion_labels\n",
    "    suggestion_labels.clear()\n",
    "    for i in range(3):\n",
    "        lbl = tk.Label(\n",
    "            panel, text=f\"{i+1}. (waiting...)\",\n",
    "            font=('Arial', 14), fg='#009688', bg='white',\n",
    "            anchor='w', justify='left'\n",
    "        )\n",
    "        lbl.pack(anchor='w', pady=2)\n",
    "        suggestion_labels.append(lbl)\n",
    "\n",
    "    return panel\n",
    "\n",
    "def pick_new_suggestions():\n",
    "    global current_suggestions\n",
    "    last_word = recorded_gestures[-1].lower() if recorded_gestures else \"\"\n",
    "\n",
    "    if last_word in bigram_dict:\n",
    "        pool = bigram_dict[last_word]\n",
    "        if len(pool) >= 3:\n",
    "            current_suggestions = random.sample(pool, 3)\n",
    "        else:\n",
    "            extra = random.sample([w for w in suggestion_words if w not in pool], 3 - len(pool))\n",
    "            current_suggestions = pool + extra\n",
    "    else:\n",
    "        current_suggestions = random.sample(suggestion_words, 3)\n",
    "\n",
    "    # update suggestion labels\n",
    "    for i, word in enumerate(current_suggestions):\n",
    "        suggestion_labels[i].config(text=f\"{i+1}. {word}\")\n",
    "\n",
    "def start_suggestion_system():\n",
    "    global suggestion_cap\n",
    "    suggestion_cap = cv2.VideoCapture(0)\n",
    "    pick_new_suggestions()\n",
    "    print(\"Suggestion system started\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3270794a-f194-482a-946d-ea2ff99ee7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loaders\n",
    "def load_model_safe(path, label_list, model_type):\n",
    "    global model, labels, current_model_type\n",
    "    if os.path.exists(path):\n",
    "        model = load_model(path)\n",
    "        labels = label_list\n",
    "        current_model_type = model_type\n",
    "        \n",
    "        messagebox.showinfo(\"Model Loaded\", f\"{model_type.upper()} model loaded.\")\n",
    "    else:\n",
    "        messagebox.showerror(\"Error\", f\"Model file not found: {path}\")\n",
    "\n",
    "def load_alphabet_model(): load_model_safe('alphabet_model.h5', ['A','B','C','D','dot','E','F','G','H','I','J','K','L','M','N','num','O','P','Q','R','S','space','T','U','V','W','words','X','Y','Z'], \"alpha\")\n",
    "def load_number_model(): load_model_safe('number_model.h5', [str(i) for i in range(10)] + [\"space\", \"alphabets\", \"words\", \"dot\"], \"num\")\n",
    "def load_word_model(): load_model_safe('singleHnad.h5', ['alphabets','dot','Hello','I love you','No','num','Ok','space','Thanks','Yes'], \"cnn\")\n",
    "def load_both_hand_model():\n",
    "    global EXPECTED_LANDMARK_SIZE\n",
    "    EXPECTED_LANDMARK_SIZE = 225 \n",
    "    load_model_safe('bothHands.h5', ['Am','Done','Good','Help','I','Later','Morning','Night','Ok','Pray','See','Want','Water','When','You'], \"mlp\")\n",
    "def load_dynamic_model(): load_model_safe('dynamic.keras', ['hello', 'my', 'name', 'is', 'i', 'study', 'in', 'department', 'am'], \"lstm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f530f473-1402-4139-94fb-74772367e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Selected: love\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Selected: student\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n"
     ]
    }
   ],
   "source": [
    "# -------MAIN GUI TESTING SCREEN CODE----------\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from tkinter import ttk\n",
    "from PIL import Image, ImageTk\n",
    "# import pyttsx3\n",
    "import time\n",
    "import os\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "import mediapipe as mp\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "\n",
    "# Constants\n",
    "imgSize = 100\n",
    "offset = 20\n",
    "EXPECTED_LANDMARK_SIZE = 1662\n",
    "sequence = deque(maxlen=40)\n",
    "\n",
    "# Globals\n",
    "running = True\n",
    "auto_speak_mode = None  # will hold StringVar from speaking settings\n",
    "model = None\n",
    "labels = []\n",
    "current_model_type = \"cnn\"\n",
    "recorded_gestures = []\n",
    "cap = None\n",
    "detector = None\n",
    "holistic = None\n",
    "caption_label = None\n",
    "suggestion_selection_active = False\n",
    "last_finger_count_time = 0\n",
    "finger_count_cooldown = 0.2  # seconds to hold\n",
    "\n",
    "\n",
    "\n",
    "# GUI elements\n",
    "root = None\n",
    "canvas = None\n",
    "text_box = None\n",
    "recorded_text = None\n",
    "auto_speak_var = None  # global toggle for auto speak\n",
    "\n",
    "\n",
    "# TTS\n",
    "# engine = None  # will initialize when user selects voice\n",
    "\n",
    " # Button colors: black buttons\n",
    "btn_color = \"#000000\"\n",
    "btn_active = \"#333333\"\n",
    "\n",
    "    # Button style\n",
    "btn_style = {\n",
    "        'font': ('Segoe UI', 14),\n",
    "        'width': 22,\n",
    "        'height': 2,\n",
    "        'bg': btn_color,\n",
    "        'fg': 'white',\n",
    "        'activebackground': btn_active,\n",
    "        'activeforeground': 'white',\n",
    "        'bd': 0,\n",
    "        'relief': 'flat',\n",
    "        'cursor': 'hand2'\n",
    "}\n",
    "\n",
    "\n",
    "# Gesture logic\n",
    "def record_gesture(gesture):\n",
    "    if gesture == \"space\":\n",
    "        recorded_gestures.append(\" \")\n",
    "    elif gesture == \"dot\":\n",
    "        speak_text()\n",
    "    elif gesture == \"num\":\n",
    "        load_number_model()\n",
    "    elif gesture == \"alphabets\":\n",
    "        load_alphabet_model()\n",
    "    elif gesture == \"words\":\n",
    "        load_word_model()\n",
    "    else:\n",
    "        recorded_gestures.append(gesture)\n",
    "        \n",
    "    update_recorded_text()\n",
    "  \n",
    "def speak_text():\n",
    "    text = recorded_text.cget(\"text\")\n",
    "    if text:\n",
    "        engine.say(text)\n",
    "        engine.runAndWait()\n",
    "\n",
    "def clear_text():\n",
    "    recorded_gestures.clear()\n",
    "    update_recorded_text()\n",
    "\n",
    "def update_recorded_text():\n",
    "    recorded_text.config(text=\" \".join(recorded_gestures))\n",
    "    pick_new_suggestions()\n",
    "    \n",
    "    # Auto speak here after text updated\n",
    "    if auto_speak_var and auto_speak_var.get():\n",
    "        if auto_speak_mode and auto_speak_mode.get() == \"full\":\n",
    "            text = recorded_text.cget(\"text\")\n",
    "            if text:\n",
    "                engine.say(text)\n",
    "                engine.runAndWait()\n",
    "        else:\n",
    "            last = recorded_gestures[-1] if recorded_gestures else \"\"\n",
    "            if last:\n",
    "                engine.say(last)\n",
    "                engine.runAndWait()\n",
    "\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "# Prediction handlers\n",
    "def predict_cnn(img):\n",
    "    global detector\n",
    "    if detector is None:\n",
    "        detector = HandDetector(maxHands=1)\n",
    "        \n",
    "    hands, img = detector.findHands((img), draw=True)  # Enable drawing for visibility\n",
    "    prediction = \"\"\n",
    "    bbox = None\n",
    "\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "        bbox = (x, y, w, h)\n",
    "\n",
    "        img_white = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "        # Safe cropping with clamped coordinates\n",
    "        y1 = max(0, y - offset)\n",
    "        y2 = min(y + h + offset, img.shape[0])\n",
    "        x1 = max(0, x - offset)\n",
    "        x2 = min(x + w + offset, img.shape[1])\n",
    "        img_crop = img[y1:y2, x1:x2]\n",
    "\n",
    "        if img_crop.size == 0:\n",
    "            return \"\"\n",
    "\n",
    "        \n",
    "\n",
    "        aspect_ratio = h / w\n",
    "        if aspect_ratio > 1:\n",
    "            k = imgSize / h\n",
    "            w_cal = math.ceil(k * w)\n",
    "            img_resize = cv2.resize(img_crop, (w_cal, imgSize))\n",
    "            w_gap = math.ceil((imgSize - w_cal) / 2)\n",
    "            img_white[:, w_gap:w_gap + w_cal] = img_resize\n",
    "        else:\n",
    "            k = imgSize / w\n",
    "            h_cal = math.ceil(k * h)\n",
    "            img_resize = cv2.resize(img_crop, (imgSize, h_cal))\n",
    "            h_gap = math.ceil((imgSize - h_cal) / 2)\n",
    "            img_white[h_gap:h_gap + h_cal, :] = img_resize\n",
    "\n",
    "        img_white = np.expand_dims(img_white / 255.0, axis=0)\n",
    "        predictions = model.predict(img_white)\n",
    "        predicted_class = np.argmax(predictions)\n",
    "        prediction = labels[predicted_class]\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def predict_mlp(img):\n",
    "    \"\"\"Predict using MLP (Words)\"\"\"\n",
    "    global holistic\n",
    "    if holistic is None:\n",
    "        holistic = mp.solutions.holistic.Holistic(static_image_mode=False, model_complexity=1, enable_segmentation=False)\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(img_rgb)\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "    if results.left_hand_landmarks:\n",
    "        for landmark in results.left_hand_landmarks.landmark:\n",
    "            landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "    if results.right_hand_landmarks:\n",
    "        for landmark in results.right_hand_landmarks.landmark:\n",
    "            landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "            \n",
    "    prediction = \"\"\n",
    "    \n",
    "    if results.left_hand_landmarks or results.right_hand_landmarks:\n",
    "        if len(landmarks) > 0:\n",
    "            landmarks = np.array(landmarks)\n",
    "            if len(landmarks) < EXPECTED_LANDMARK_SIZE:\n",
    "                landmarks = np.pad(landmarks, (0, EXPECTED_LANDMARK_SIZE - len(landmarks)))\n",
    "            elif len(landmarks) > EXPECTED_LANDMARK_SIZE:\n",
    "                landmarks = landmarks[:EXPECTED_LANDMARK_SIZE]\n",
    "\n",
    "            predictions = model.predict(landmarks.reshape(1, -1))\n",
    "            predicted_class = np.argmax(predictions)\n",
    "            prediction = labels[predicted_class]\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def predict_lstm(img):\n",
    "    global holistic, sequence\n",
    "    if holistic is None:\n",
    "        holistic = mp.solutions.holistic.Holistic()\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(img_rgb)\n",
    "    keypoints = extract_keypoints(results)\n",
    "    sequence.append(keypoints)\n",
    "    if len(sequence) == 40:\n",
    "        prediction = model.predict(np.expand_dims(sequence, axis=0))\n",
    "        return labels[np.argmax(prediction)]\n",
    "    return \"\"\n",
    "\n",
    "# Main camera loop\n",
    "def update_frame():\n",
    "    global cap,running\n",
    "    if not hasattr(update_frame, 'last_finger_count'):\n",
    "        update_frame.last_finger_count = None\n",
    "        update_frame.last_finger_count_time = time.time()\n",
    "    success, img = cap.read()\n",
    "    if not running:\n",
    "        return\n",
    "    if not success:\n",
    "        print(\"Camera failed.\")\n",
    "        return\n",
    "\n",
    "    gesture = \"\"\n",
    "    landmarks_drawn = False  # Flag to control drawing\n",
    "\n",
    "    if current_model_type in [\"cnn\", \"num\",'alpha']:\n",
    "       gesture = predict_cnn(img)\n",
    "\n",
    "    elif current_model_type == \"mlp\":\n",
    "        gesture = predict_mlp(img)\n",
    "\n",
    "    elif current_model_type in [\"lstm\"]:\n",
    "        if holistic is None:\n",
    "            initialize_holistic = mp.solutions.holistic.Holistic()\n",
    "        else:\n",
    "            initialize_holistic = holistic\n",
    "\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = initialize_holistic.process(img_rgb)\n",
    "\n",
    "        # Draw landmarks\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "        mp_drawing.draw_landmarks(img, results.pose_landmarks, mp.solutions.holistic.POSE_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(img, results.left_hand_landmarks, mp.solutions.holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(img, results.right_hand_landmarks, mp.solutions.holistic.HAND_CONNECTIONS)\n",
    "        mp_drawing.draw_landmarks(img, results.face_landmarks, mp.solutions.holistic.FACEMESH_TESSELATION)\n",
    "        landmarks_drawn = True\n",
    "\n",
    "        if current_model_type == \"lstm\":\n",
    "            keypoints = extract_keypoints(results)\n",
    "            sequence.append(keypoints)\n",
    "            if len(sequence) == 40:\n",
    "                prediction = model.predict(np.expand_dims(sequence, axis=0))\n",
    "                gesture = labels[np.argmax(prediction)]\n",
    "\n",
    "    if gesture:\n",
    "        text_box.delete(0, tk.END)\n",
    "        text_box.insert(0, gesture)\n",
    "\n",
    "        if hasattr(update_frame, \"last_gesture\") and gesture == update_frame.last_gesture:\n",
    "            if time.time() - update_frame.last_time >= 2:\n",
    "                record_gesture(gesture)\n",
    "                update_frame.last_time = time.time()\n",
    "        else:\n",
    "            update_frame.last_gesture = gesture\n",
    "            update_frame.last_time = time.time()\n",
    "\n",
    "    # Draw gesture text on the webcam frame\n",
    "    if gesture:\n",
    "        cv2.putText(img, f\"Prediction: {gesture}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (50, 255, 50), 2)\n",
    "\n",
    "    # Finger count selection\n",
    "    if current_model_type in [\"cnn\", \"mlp\"]:\n",
    "       global detector,last_suggestion_time,suggestion_selection_active\n",
    "       if detector is None:\n",
    "          detector = HandDetector(maxHands=1)\n",
    "       hands, _ = detector.findHands(img, draw=False)\n",
    "       if hands:\n",
    "          hand = hands[0]\n",
    "          fingers = detector.fingersUp(hand)\n",
    "          count = fingers.count(1)\n",
    "          now = time.time()\n",
    "          if count in [1,2,3]:\n",
    "            # check if same count is held for cooldown time\n",
    "            if hasattr(update_frame, 'last_finger_count') and update_frame.last_finger_count == count:\n",
    "                if time.time() - update_frame.last_finger_count_time >= finger_count_cooldown:\n",
    "                   if not suggestion_selection_active:\n",
    "                      idx = count - 1\n",
    "                      if idx < len(current_suggestions):\n",
    "                         word = current_suggestions[idx]\n",
    "                         recorded_gestures.append(word)\n",
    "                         update_recorded_text()\n",
    "                         print(f\"Selected: {word}\")\n",
    "                         pick_new_suggestions()\n",
    "                         suggestion_selection_active = True\n",
    "            else:\n",
    "               # new finger count detected, reset timer\n",
    "               update_frame.last_finger_count = count\n",
    "               update_frame.last_finger_count_time = time.time()\n",
    "          else:\n",
    "             suggestion_selection_active = False\n",
    "             update_frame.last_finger_count = None\n",
    "\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=imgtk)\n",
    "    canvas.imgtk = imgtk\n",
    "    \n",
    "    root.after(10, update_frame)\n",
    "\n",
    "       \n",
    "\n",
    "# GUI setup\n",
    "def start_gui():\n",
    "    global root, cap, canvas, text_box, recorded_text\n",
    "    running = True\n",
    "    root = tk.Tk()\n",
    "    root.title(\"ASL Recognition - All Models\")\n",
    "    root.attributes('-fullscreen', True)\n",
    "\n",
    "    root.bind('<Escape>', lambda event: root.attributes('-fullscreen', False))\n",
    "\n",
    "    # Create main frame horizontally\n",
    "    main_frame = tk.Frame(root, bg=\"#f0f0f0\")\n",
    "    main_frame.pack(pady=10, expand=True)\n",
    "\n",
    "    # Add some stretch space on the left to center the canvas\n",
    "    left_spacer = tk.Frame(main_frame, width=250, bg=\"#f0f0f0\")\n",
    "    left_spacer.pack(side=tk.LEFT, fill=tk.Y)\n",
    "\n",
    "    # Canvas in the center\n",
    "    canvas = tk.Canvas(main_frame, width=640, height=400, bg='black')\n",
    "    canvas.pack(side=tk.LEFT)\n",
    "\n",
    "    # Suggestions panel on the right\n",
    "    suggestions_panel = create_suggestions_panel(main_frame)\n",
    "    suggestions_panel.pack(side=tk.LEFT, padx=20)\n",
    "    pick_new_suggestions()\n",
    "    \n",
    "    text_frame = tk.Frame(root, bg=\"#f0f0f0\")\n",
    "    text_frame.pack(pady=10)\n",
    "\n",
    "    text_box = tk.Entry(text_frame, font=('Arial', 24), width=10)\n",
    "    text_box.pack(side=tk.LEFT, padx=(0,10))\n",
    "\n",
    "    recorded_text = tk.Label(root, text=\"\", font=('Arial', 14), relief=\"solid\", width=20, height=2, wraplength=300)\n",
    "    recorded_text.pack(pady=10)\n",
    "    \n",
    "\n",
    "    setup_buttons()\n",
    "\n",
    "    # global suggestion_label\n",
    "    # suggestion_label = tk.Label(root, text=\"\", font=('Arial', 14), fg=\"blue\")\n",
    "    # suggestion_label.pack(pady=10)\n",
    "    # pick_new_suggestions()\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        messagebox.showerror(\"Error\", \"Cannot access webcam.\")\n",
    "        return\n",
    "\n",
    "    update_frame()\n",
    "    root.mainloop()\n",
    "    try:\n",
    "        if cap:\n",
    "            cap.release()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Initial model selector\n",
    "import tkinter as tk\n",
    "\n",
    "def model_selector():\n",
    "    sel_root = tk.Tk()\n",
    "    sel_root.title(\"🤟 Choose ASL Model\")\n",
    "    sel_root.geometry(\"500x520\")\n",
    "    sel_root.configure(bg=\"#ffffff\")  # pure white background\n",
    "\n",
    "    # Heading label\n",
    "    label = tk.Label(\n",
    "        sel_root,\n",
    "        text=\"Select Initial Model\",\n",
    "        font=('Segoe UI', 20, 'bold'),\n",
    "        bg=\"#ffffff\",\n",
    "        fg=\"#000000\"\n",
    "    )\n",
    "    label.pack(pady=(40, 25))\n",
    "\n",
    "   \n",
    "\n",
    "    # Buttons with emojis\n",
    "    tk.Button(sel_root, text=\"🔤 Alphabets\", command=lambda: [load_alphabet_model(), sel_root.destroy(), start_gui()], **btn_style).pack(pady=8)\n",
    "    tk.Button(sel_root, text=\"🔢 Numbers\", command=lambda: [load_number_model(), sel_root.destroy(), start_gui()], **btn_style).pack(pady=8)\n",
    "    tk.Button(sel_root, text=\"📝 Words\", command=lambda: [load_word_model(), sel_root.destroy(), start_gui()], **btn_style).pack(pady=8)\n",
    "    tk.Button(sel_root, text=\"✋✋ Both Hands\", command=lambda: [load_both_hand_model(), sel_root.destroy(), start_gui()], **btn_style).pack(pady=8)\n",
    "    tk.Button(sel_root, text=\"⚡ Dynamic\", command=lambda: [load_dynamic_model(), sel_root.destroy(), start_gui()], **btn_style).pack(pady=8)\n",
    "\n",
    "    sel_root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_selector()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83240512-41c9-4d79-9ae8-280b30c5ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import threading\n",
    "import speech_recognition as sr\n",
    "\n",
    "def start_listen_gui():\n",
    "    root = tk.Tk()\n",
    "    root.title(\"🎤 Voice Recognition\")\n",
    "    root.geometry(\"500x300\")\n",
    "    root.resizable(False, False)\n",
    "    root.configure(bg='#f0f4f8')  # soft background\n",
    "\n",
    "    frame = tk.Frame(root, bg='#f0f4f8')\n",
    "    frame.pack(expand=True)\n",
    "\n",
    "    # Stylish title (static, no blink)\n",
    "    title = tk.Label(frame, text=\"🎤 Listening... Say 'exit' to stop\",\n",
    "                     font=('Segoe UI', 18, 'bold'), bg='#f0f4f8', fg='#333')\n",
    "    title.pack(pady=(20,10))\n",
    "\n",
    "    # Text display with bold font\n",
    "    text_display = tk.Label(frame, text=\"📝 Waiting for speech...\",\n",
    "                            font=('Segoe UI', 14, 'bold'), wraplength=450,\n",
    "                            bg='white', fg='#444', width=40, height=5,\n",
    "                            bd=2, relief=\"groove\", justify=\"center\")\n",
    "    text_display.pack(pady=15, padx=20)\n",
    "\n",
    "    stop_flag = threading.Event()\n",
    "\n",
    "    def safe_update_text(new_text):\n",
    "        if text_display.winfo_exists():\n",
    "            text_display.config(text=new_text)\n",
    "\n",
    "    def listen_loop():\n",
    "        recognizer = sr.Recognizer()\n",
    "        mic = sr.Microphone()\n",
    "        with mic as source:\n",
    "            recognizer.adjust_for_ambient_noise(source)\n",
    "            while not stop_flag.is_set():\n",
    "                try:\n",
    "                    audio = recognizer.listen(source, timeout=10, phrase_time_limit=5)\n",
    "                    text = recognizer.recognize_google(audio)\n",
    "\n",
    "                    root.after(0, lambda: safe_update_text(f\"📝 {text}\"))\n",
    "\n",
    "                    if text.lower() in [\"quit\", \"exit\", \"stop\"]:\n",
    "                        stop_flag.set()\n",
    "                        root.after(0, lambda: safe_update_text(\"👋 Returning to home...\"))\n",
    "                        def go_home():\n",
    "                            try:\n",
    "                               root.destroy()\n",
    "                            except: pass\n",
    "                            model_selector()  # go back to home screen\n",
    "                        root.after(1000, go_home)\n",
    "                        break\n",
    "\n",
    "\n",
    "                except sr.WaitTimeoutError:\n",
    "                    root.after(0, lambda: safe_update_text(\"⏸️ No speech detected.\"))\n",
    "                    break\n",
    "                except sr.UnknownValueError:\n",
    "                    root.after(0, lambda: safe_update_text(\"🤔 Couldn't understand audio.\"))\n",
    "                except sr.RequestError:\n",
    "                    root.after(0, lambda: safe_update_text(\"❌ Check your internet connection.\"))\n",
    "                    break\n",
    "\n",
    "    threading.Thread(target=listen_loop, daemon=True).start()\n",
    "    root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "711bd13f-25a3-4872-b829-69325c2e14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------VOICE SETTING GUI AND LOGIC----------\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "default_rate = engine.getProperty('rate')\n",
    "\n",
    "def blend_colors(color1, color2, factor):\n",
    "    def hex_to_rgb(hex):\n",
    "        hex = hex.lstrip('#')\n",
    "        return tuple(int(hex[i:i+2], 16) for i in (0, 2, 4))\n",
    "    def rgb_to_hex(rgb):\n",
    "        return '#{:02x}{:02x}{:02x}'.format(*rgb)\n",
    "    rgb1 = hex_to_rgb(color1)\n",
    "    rgb2 = hex_to_rgb(color2)\n",
    "    blended = tuple(int(a + (b - a) * factor) for a, b in zip(rgb1, rgb2))\n",
    "    return rgb_to_hex(blended)\n",
    "\n",
    "def select_exact_voice_and_start_gui():\n",
    "    global engine, cap, root\n",
    "    try:\n",
    "        if cap:\n",
    "            cap.release()\n",
    "            cap = None\n",
    "    except: pass\n",
    "    try:\n",
    "        if root:\n",
    "            root.destroy()\n",
    "            root = None\n",
    "    except: pass\n",
    "\n",
    "    voices = engine.getProperty('voices')\n",
    "    voice_names = [v.name for v in voices]\n",
    "\n",
    "    sel_win = tk.Tk()\n",
    "    sel_win.title(\"🎙️ Select Voice & Settings\")\n",
    "    sel_win.geometry(\"440x420\")\n",
    "    sel_win.configure(bg=\"#ffffff\")\n",
    "\n",
    "    # --- Title ---\n",
    "    tk.Label(sel_win, text=\"Choose Voice to Speak\", font=('Segoe UI', 18, 'bold'),\n",
    "             bg='#ffffff').pack(pady=(20, 12))\n",
    "\n",
    "    # --- Voice selection combobox ---\n",
    "    voice_var = tk.StringVar(value=voice_names[0])\n",
    "    style = ttk.Style(sel_win)\n",
    "    style.theme_use('default')\n",
    "    style.configure(\"TCombobox\", font=('Segoe UI', 14))\n",
    "    ttk.Combobox(sel_win, textvariable=voice_var, state=\"readonly\",\n",
    "                 values=voice_names, width=30).pack(pady=5)\n",
    "\n",
    "    # --- Auto speak toggle (better visibility & centered) ---\n",
    "    auto_speak_var_local = tk.BooleanVar(value=True)\n",
    "\n",
    "    toggle_frame = tk.Frame(sel_win, bg='#ffffff')\n",
    "    toggle_frame.pack(pady=5)\n",
    "\n",
    "    auto_speak_chk = tk.Checkbutton(\n",
    "        toggle_frame,\n",
    "        text=\"🔊 Enable Auto Speak\",\n",
    "        variable=auto_speak_var_local,\n",
    "        onvalue=True, offvalue=False,\n",
    "        font=('Segoe UI', 13, 'bold'),\n",
    "        bg='#ffffff',\n",
    "        activebackground='#ffffff',\n",
    "        relief='solid',\n",
    "        bd=1,\n",
    "        highlightthickness=1,\n",
    "        padx=12,\n",
    "        pady=6\n",
    "    )\n",
    "    auto_speak_chk.pack()\n",
    "\n",
    "\n",
    "    # --- Auto speak mode (last gesture / full text) ---\n",
    "    auto_speak_mode_local = tk.StringVar(value=\"last\")\n",
    "    tk.Label(sel_win, text=\"Auto Speak Mode:\", font=('Segoe UI', 13, 'bold'),\n",
    "             bg='#ffffff').pack(pady=(12, 3))\n",
    "    mode_frame = tk.Frame(sel_win, bg='#ffffff')\n",
    "    mode_frame.pack(pady=3)\n",
    "    tk.Radiobutton(mode_frame, text=\"🗣️ Last Gesture\", variable=auto_speak_mode_local,\n",
    "                   value=\"last\", font=('Segoe UI', 11), bg='#ffffff',\n",
    "                   activebackground='#ffffff').pack(side=tk.LEFT, padx=10)\n",
    "    tk.Radiobutton(mode_frame, text=\"📢 Full Text\", variable=auto_speak_mode_local,\n",
    "                   value=\"full\", font=('Segoe UI', 11), bg='#ffffff',\n",
    "                   activebackground='#ffffff').pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "    # --- Speech speed slider ---\n",
    "    tk.Label(sel_win, text=\"Speech Speed:\", font=('Segoe UI', 13, 'bold'),\n",
    "             bg='#ffffff').pack(pady=(18, 3))\n",
    "    speed_var = tk.IntVar(value=default_rate)\n",
    "    speed_display = tk.Label(sel_win, text=f\"{default_rate} WPM\",\n",
    "                             font=('Segoe UI', 12), bg='#ffffff', fg='#00b894')\n",
    "    speed_display.pack(pady=2)\n",
    "\n",
    "    color_stops = [\n",
    "        (100, '#00b894'),   # teal\n",
    "        (120, '#74b9ff'),   # sky blue\n",
    "        (160, '#a29bfe'),   # lilac\n",
    "        (180, '#dfe6e9'),   # light gray\n",
    "        (200, '#636e72'),   # dark gray\n",
    "        (220, '#000000')    # black\n",
    "    ]\n",
    "\n",
    "    def get_gradient_color(speed):\n",
    "        for i in range(len(color_stops)-1):\n",
    "            s1, c1 = color_stops[i]\n",
    "            s2, c2 = color_stops[i+1]\n",
    "            if s1 <= speed <= s2:\n",
    "                factor = (speed - s1) / (s2 - s1)\n",
    "                return blend_colors(c1, c2, factor)\n",
    "        return color_stops[-1][1]\n",
    "\n",
    "    def on_speed_change(val):\n",
    "        v = int(float(val))\n",
    "        speed_display.config(text=f\"{v} WPM\")\n",
    "        new_color = get_gradient_color(v)\n",
    "        style.configure(\"Custom.Horizontal.TScale\",\n",
    "                        background=new_color, troughcolor=new_color)\n",
    "        speed_display.config(fg=new_color)\n",
    "\n",
    "    style.configure(\"Custom.Horizontal.TScale\", thickness=10,\n",
    "                    background='#00b894', troughcolor='#00b894')\n",
    "    ttk.Scale(sel_win, from_=100, to=240, orient='horizontal',\n",
    "              variable=speed_var, command=on_speed_change,\n",
    "              length=260, style=\"Custom.Horizontal.TScale\").pack(pady=5)\n",
    "\n",
    "    # --- Continue button ---\n",
    "    def start_with_selected_voice():\n",
    "        global auto_speak_var, auto_speak_mode\n",
    "        selected = voice_var.get()\n",
    "        for v in voices:\n",
    "            if selected == v.name:\n",
    "                engine.setProperty('voice', v.id)\n",
    "                break\n",
    "        engine.setProperty('rate', speed_var.get())\n",
    "        auto_speak_var = auto_speak_var_local\n",
    "        auto_speak_mode = auto_speak_mode_local\n",
    "        sel_win.destroy()\n",
    "        model_selector()\n",
    "\n",
    "    tk.Button(sel_win, text=\"✅ Continue\", command=start_with_selected_voice,\n",
    "              font=('Segoe UI', 14, 'bold'), bg='#000000', fg='white',\n",
    "              activebackground='#333333', activeforeground='white',\n",
    "              bd=0, relief='flat', cursor='hand2', width=18, height=2).pack(pady=20)\n",
    "\n",
    "    sel_win.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf3c8d2e-529c-44d1-9a3d-223431b8d44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------FIRST SCREEN WITH ANIMATION----------\n",
    "\n",
    "\n",
    "import tkinter as tk\n",
    "\n",
    "def open_speak():\n",
    "    model_selector()  # define elsewhere\n",
    "\n",
    "def open_listen():\n",
    "    start_listen_gui()  # define elsewhere\n",
    "\n",
    "def main_screen():\n",
    "    root = tk.Tk()\n",
    "    root.title(\"🤟 ASL Recognition System\")\n",
    "    root.geometry(\"600x500\")\n",
    "    root.resizable(False, False)\n",
    "    root.configure(bg='#f9f9f9')\n",
    "\n",
    "    frame = tk.Frame(root, bg='#f9f9f9')\n",
    "    frame.pack(expand=True)\n",
    "\n",
    "    label = tk.Label(frame, text=\"Welcome to ASL Recognition\",\n",
    "                     font=('Segoe UI', 24, 'bold'), bg='#f9f9f9', fg='#333')\n",
    "    label.pack(pady=30)\n",
    "\n",
    "    font_family = 'Segoe UI'\n",
    "\n",
    "    # Start small, remember current sizes\n",
    "    btn_listen_font_size = 1\n",
    "    btn_speak_font_size = 1\n",
    "\n",
    "    btn_listen = tk.Button(frame, text=\"🎧 Listen\", font=(font_family, btn_listen_font_size, 'bold'),\n",
    "                           bg='#00b894', fg='white',\n",
    "                           activebackground='#019875', activeforeground='white',\n",
    "                           width=1, height=1, bd=0, relief='flat', cursor='hand2',\n",
    "                           command=lambda: [root.destroy(), open_listen()])\n",
    "    btn_listen.pack(pady=12) \n",
    "\n",
    "    btn_speak = tk.Button(frame, text=\"🤟 Speak (Sign)\", font=(font_family, btn_speak_font_size, 'bold'),\n",
    "                          bg='#0984e3', fg='white',\n",
    "                          activebackground='#086fc1', activeforeground='white',\n",
    "                          width=1, height=1, bd=0, relief='flat', cursor='hand2',\n",
    "                          command=lambda: [root.destroy(), select_exact_voice_and_start_gui()])\n",
    "    btn_speak.pack(pady=12)\n",
    "\n",
    "    # Animate: just keep track of current size ourselves\n",
    "    def animate_button(widget, current_size, target_size, target_width, target_height, step=1):\n",
    "        if current_size < target_size:\n",
    "            current_size += step\n",
    "            widget.config(\n",
    "                font=(font_family, current_size, 'bold'),\n",
    "                width=min(target_width, widget.cget('width') + 1),\n",
    "                height=min(target_height, widget.cget('height') + 1)\n",
    "            )\n",
    "            root.after(15, lambda: animate_button(widget, current_size, target_size, target_width, target_height, step))\n",
    "\n",
    "    root.after(300, lambda: animate_button(btn_listen, btn_listen_font_size, target_size=16, target_width=16, target_height=2))\n",
    "    root.after(600, lambda: animate_button(btn_speak, btn_speak_font_size, target_size=16, target_width=16, target_height=2))\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "main_screen()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49b4a4c-abcd-4426-9dc2-d5fbe19111ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Hazel Desktop - English (Great Britain)\n",
      "Microsoft David Desktop - English (United States)\n",
      "Microsoft Zira Desktop - English (United States)\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()\n",
    "for voice in engine.getProperty('voices'):\n",
    "    print(voice.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b14fe-c2a4-4baa-ad9f-7e6deb8ae124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
