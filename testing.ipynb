{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6132891d-666d-4b52-b994-f56059a72820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct code\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import pyttsx3\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ===== Constants =====\n",
    "IMG_SIZE = 100\n",
    "OFFSET = 20\n",
    "EXPECTED_LANDMARK_SIZE = 225  # For Words model (75 landmarks * 3 values)\n",
    "\n",
    "# ===== Global Variables =====\n",
    "current_model = None\n",
    "model = None\n",
    "labels = []\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "recorded_gestures = []\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 150)  # Slower speech rate\n",
    "\n",
    "# Initialize MediaPipe Holistic (for Words model)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# ===== GUI Setup =====\n",
    "root = tk.Tk()\n",
    "root.title(\"ASL Recognition System\")\n",
    "root.geometry(\"1000x800\")\n",
    "\n",
    "# Webcam display\n",
    "canvas = tk.Canvas(root, width=640, height=480)\n",
    "canvas.pack(pady=10)\n",
    "\n",
    "# Recognized gesture text box\n",
    "text_box = tk.Entry(root, font=('Arial', 24), width=20)\n",
    "text_box.pack(pady=10)\n",
    "\n",
    "# Recorded gestures label\n",
    "recorded_label = tk.Label(root, text=\"Recorded Gestures:\", font=('Arial', 14))\n",
    "recorded_label.pack(pady=5)\n",
    "recorded_text = tk.Label(root, text=\"\", font=('Arial', 14), wraplength=300, height=4, relief=\"solid\")\n",
    "recorded_text.pack(pady=5)\n",
    "\n",
    "# Button frame\n",
    "button_frame = tk.Frame(root)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "# Mode switch frame\n",
    "mode_frame = tk.Frame(root)\n",
    "mode_frame.pack(pady=10)\n",
    "\n",
    "# ===== Core Functions =====\n",
    "def load_model_cnn(model_path, label_list):\n",
    "    \"\"\"Load CNN model for Alphabets/Numbers\"\"\"\n",
    "    global model, labels, current_model\n",
    "    if os.path.exists(model_path):\n",
    "        model = load_model(model_path)\n",
    "        labels = label_list\n",
    "        current_model = \"cnn\"\n",
    "        messagebox.showinfo(\"Model Loaded\", f\"Switched to {'Alphabets' if label_list[0].isalpha() else 'Numbers'} mode\")\n",
    "    else:\n",
    "        messagebox.showerror(\"Error\", \"Model file not found!\")\n",
    "\n",
    "def load_model_mlp(model_path, label_list):\n",
    "    \"\"\"Load MLP model for Words\"\"\"\n",
    "    global model, labels, current_model\n",
    "    if os.path.exists(model_path):\n",
    "        model = load_model(model_path)\n",
    "        labels = label_list\n",
    "        current_model = \"mlp\"\n",
    "        messagebox.showinfo(\"Model Loaded\", \"Switched to Words mode\")\n",
    "    else:\n",
    "        messagebox.showerror(\"Error\", \"Model file not found!\")\n",
    "\n",
    "def predict_cnn(img):\n",
    "    \"\"\"Predict using CNN (Alphabets/Numbers) with bounding box\"\"\"\n",
    "    hands, img = detector.findHands(img)\n",
    "    prediction = \"\"\n",
    "    bbox = None\n",
    "\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "        bbox = (x, y, w, h)\n",
    "        img_white = np.ones((IMG_SIZE, IMG_SIZE, 3), np.uint8) * 255\n",
    "        img_crop = img[max(0, y - OFFSET):min(y + h + OFFSET, img.shape[0]),\n",
    "                       max(0, x - OFFSET):min(x + w + OFFSET, img.shape[1])]\n",
    "        try:\n",
    "            aspect_ratio = h / w\n",
    "            if aspect_ratio > 1:\n",
    "                k = IMG_SIZE / h\n",
    "                w_cal = math.ceil(k * w)\n",
    "                img_resize = cv2.resize(img_crop, (w_cal, IMG_SIZE))\n",
    "                w_gap = math.ceil((IMG_SIZE - w_cal) / 2)\n",
    "                img_white[:, w_gap:w_gap + w_cal] = img_resize\n",
    "            else:\n",
    "                k = IMG_SIZE / w\n",
    "                h_cal = math.ceil(k * h)\n",
    "                img_resize = cv2.resize(img_crop, (IMG_SIZE, h_cal))\n",
    "                h_gap = math.ceil((IMG_SIZE - h_cal) / 2)\n",
    "                img_white[h_gap:h_gap + h_cal, :] = img_resize\n",
    "\n",
    "            img_white = np.expand_dims(img_white / 255.0, axis=0)\n",
    "            predictions = model.predict(img_white)\n",
    "            predicted_class = np.argmax(predictions)\n",
    "            prediction = labels[predicted_class]\n",
    "        except Exception as e:\n",
    "            print(\"Prediction error:\", e)\n",
    "\n",
    "    return prediction, bbox\n",
    "\n",
    "\n",
    "def predict_mlp(img):\n",
    "    \"\"\"Predict using MLP (Words)\"\"\"\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(img_rgb)\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "    if results.left_hand_landmarks:\n",
    "        for landmark in results.left_hand_landmarks.landmark:\n",
    "            landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "    if results.right_hand_landmarks:\n",
    "        for landmark in results.right_hand_landmarks.landmark:\n",
    "            landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "    \n",
    "    prediction = \"\"\n",
    "    if len(landmarks) > 0:\n",
    "        landmarks = np.array(landmarks)\n",
    "        if len(landmarks) < EXPECTED_LANDMARK_SIZE:\n",
    "            landmarks = np.pad(landmarks, (0, EXPECTED_LANDMARK_SIZE - len(landmarks)))\n",
    "        elif len(landmarks) > EXPECTED_LANDMARK_SIZE:\n",
    "            landmarks = landmarks[:EXPECTED_LANDMARK_SIZE]\n",
    "        \n",
    "        predictions = model.predict(landmarks.reshape(1, -1))\n",
    "        predicted_class = np.argmax(predictions)\n",
    "        prediction = labels[predicted_class]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def update_frame():\n",
    "    \"\"\"Update webcam feed and predictions\"\"\"\n",
    "    global recorded_gestures\n",
    "    \n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        return\n",
    "    \n",
    "    img_output = img.copy()\n",
    "    prediction = \"\"\n",
    "    mode_text = \"\"\n",
    "    \n",
    "    if current_model == \"cnn\":\n",
    "        prediction, bbox = predict_cnn(img)\n",
    "        mode_text = f\"Mode: {'Alphabets' if labels[0].isalpha() else 'Numbers'}\"\n",
    "        \n",
    "        # Draw bounding box and prediction if hand detected\n",
    "        if bbox:\n",
    "            x, y, w, h = bbox\n",
    "            cv2.rectangle(img_output, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img_output, prediction, (x, y - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "    elif current_model == \"mlp\":\n",
    "        prediction = predict_mlp(img)\n",
    "        mode_text = \"Mode: Words\"\n",
    "    \n",
    "    # Display mode and update text box\n",
    "    cv2.putText(img_output, mode_text, (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    if prediction:\n",
    "        text_box.delete(0, tk.END)\n",
    "        text_box.insert(0, prediction)\n",
    "    \n",
    "    # Display webcam feed\n",
    "    img_output = cv2.cvtColor(img_output, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_output)\n",
    "    img_tk = ImageTk.PhotoImage(image=img_pil)\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=img_tk)\n",
    "    canvas.img_tk = img_tk\n",
    "    \n",
    "    root.after(10, update_frame)\n",
    "\n",
    "# ===== GUI Components =====\n",
    "# Control buttons\n",
    "tk.Button(button_frame, text=\"Record Gesture\", font=('Arial', 14), \n",
    "          command=lambda: recorded_gestures.append(text_box.get()) or update_recorded_text(), \n",
    "          bg=\"lightblue\").pack(side=tk.LEFT, padx=5)\n",
    "tk.Button(button_frame, text=\"Speak\", font=('Arial', 14), \n",
    "          command=lambda: engine.say(\" \".join(recorded_gestures)) or engine.runAndWait(), \n",
    "          bg=\"lightgreen\").pack(side=tk.LEFT, padx=5)\n",
    "tk.Button(button_frame, text=\"Clear\", font=('Arial', 14), \n",
    "          command=lambda: recorded_gestures.clear() or update_recorded_text(), \n",
    "          bg=\"salmon\").pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Model switch buttons\n",
    "tk.Button(mode_frame, text=\"Alphabets\", font=('Arial', 14), \n",
    "          command=lambda: load_model_cnn('asl_alphabet_model.h5', list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")), \n",
    "          bg=\"lightyellow\").pack(side=tk.LEFT, padx=5)\n",
    "tk.Button(mode_frame, text=\"Numbers\", font=('Arial', 14), \n",
    "          command=lambda: load_model_cnn('asl_numeric_model.h5', list(\"0123456789\")), \n",
    "          bg=\"lightyellow\").pack(side=tk.LEFT, padx=5)\n",
    "tk.Button(mode_frame, text=\"Words\", font=('Arial', 14), \n",
    "          command=lambda: load_model_mlp('best_asl_model2.keras', [\n",
    "              'Bathroom', 'Call', 'Done', 'Drink', 'Eat', 'Father', 'Friend', \n",
    "              'Good', 'Hello', 'Help', 'I', 'I love you', 'Later', 'More', \n",
    "              'Morning', 'Mother', 'Need', 'Night', 'No', 'Ok', 'Pain', 'Peace',\n",
    "              'Please', 'Pray', 'Repeat', 'See', 'Silence', 'Sorry', 'Stop', \n",
    "              'Thank you', 'Want', 'Water', 'What', 'When', 'Where', 'Who', \n",
    "              'Why', 'Yes', 'You']), \n",
    "          bg=\"lightyellow\").pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "def update_recorded_text():\n",
    "    \"\"\"Update the recorded gestures display\"\"\"\n",
    "    recorded_text.config(text=\" \".join(recorded_gestures))\n",
    "\n",
    "# Initialize with Alphabets model by default\n",
    "load_model_cnn('asl_alphabet_model.h5', list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "\n",
    "# Start the frame update loop\n",
    "update_frame()\n",
    "\n",
    "# Close handler\n",
    "def on_closing():\n",
    "    cap.release()\n",
    "    root.destroy()\n",
    "\n",
    "root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b8d91e-7a96-4f7f-8979-ac66db5dd210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import pyttsx3\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# ===== Constants =====\n",
    "IMG_SIZE = 100\n",
    "OFFSET = 20\n",
    "EXPECTED_LANDMARK_SIZE = 225  # For Words model (75 landmarks * 3 values)\n",
    "\n",
    "# ===== Global Variables =====\n",
    "current_model = None\n",
    "model = None\n",
    "labels = []\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "recorded_gestures = []\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('rate', 150)  # Slower speech rate\n",
    "\n",
    "# Initialize MediaPipe Holistic (for Words model)\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# ===== GUI Setup =====\n",
    "root = tk.Tk()\n",
    "root.title(\"ASL Recognition System\")\n",
    "root.geometry(\"1000x800\")\n",
    "\n",
    "# Webcam display\n",
    "canvas = tk.Canvas(root, width=640, height=480)\n",
    "canvas.pack(pady=10)\n",
    "\n",
    "# Recognized gesture text box\n",
    "text_box = tk.Entry(root, font=('Arial', 24), width=20)\n",
    "text_box.pack(pady=10)\n",
    "\n",
    "# Recorded gestures label\n",
    "recorded_label = tk.Label(root, text=\"Recorded Gestures:\", font=('Arial', 14))\n",
    "recorded_label.pack(pady=5)\n",
    "recorded_text = tk.Label(root, text=\"\", font=('Arial', 14), wraplength=300, height=4, relief=\"solid\")\n",
    "recorded_text.pack(pady=5)\n",
    "\n",
    "# Button frame\n",
    "button_frame = tk.Frame(root)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "# Mode switch frame\n",
    "mode_frame = tk.Frame(root)\n",
    "mode_frame.pack(pady=10)\n",
    "\n",
    "# ===== Core Functions =====\n",
    "def load_model_cnn(model_path, label_list):\n",
    "    \"\"\"Load CNN model for Alphabets/Numbers\"\"\"\n",
    "    global model, labels, current_model\n",
    "    if os.path.exists(model_path):\n",
    "        model = load_model(model_path)\n",
    "        labels = label_list\n",
    "        current_model = \"cnn\"\n",
    "        messagebox.showinfo(\"Model Loaded\", f\"Switched to {'Alphabets' if label_list[0].isalpha() else 'Numbers'} mode\")\n",
    "    else:\n",
    "        messagebox.showerror(\"Error\", \"Model file not found!\")\n",
    "\n",
    "def load_model_mlp(model_path, label_list):\n",
    "    \"\"\"Load MLP model for Words\"\"\"\n",
    "    global model, labels, current_model\n",
    "    if os.path.exists(model_path):\n",
    "        model = load_model(model_path)\n",
    "        labels = label_list\n",
    "        current_model = \"mlp\"\n",
    "        messagebox.showinfo(\"Model Loaded\", \"Switched to Words mode\")\n",
    "    else:\n",
    "        messagebox.showerror(\"Error\", \"Model file not found!\")\n",
    "\n",
    "def predict_cnn(img):\n",
    "    \"\"\"Predict using CNN (Alphabets/Numbers) with bounding box\"\"\"\n",
    "    hands, img = detector.findHands(img, draw=False)  # We'll draw the box ourselves\n",
    "    prediction = \"\"\n",
    "    bbox = None\n",
    "    \n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "        bbox = (x, y, w, h)\n",
    "        \n",
    "        img_crop = img[max(0, y-OFFSET):min(y+h+OFFSET, img.shape[0]),\n",
    "                      max(0, x-OFFSET):min(x+w+OFFSET, img.shape[1])]\n",
    "        \n",
    "        img_white = np.ones((IMG_SIZE, IMG_SIZE, 3), np.uint8) * 255\n",
    "        aspect_ratio = h / w\n",
    "        \n",
    "        try:\n",
    "            if aspect_ratio > 1:\n",
    "                k = IMG_SIZE / h\n",
    "                w_cal = math.ceil(k * w)\n",
    "                img_resize = cv2.resize(img_crop, (w_cal, IMG_SIZE))\n",
    "                w_gap = math.ceil((IMG_SIZE - w_cal) / 2)\n",
    "                img_white[:, w_gap:w_gap+w_cal] = img_resize\n",
    "            else:\n",
    "                k = IMG_SIZE / w\n",
    "                h_cal = math.ceil(k * h)\n",
    "                img_resize = cv2.resize(img_crop, (IMG_SIZE, h_cal))\n",
    "                h_gap = math.ceil((IMG_SIZE - h_cal) / 2)\n",
    "                img_white[h_gap:h_gap+h_cal, :] = img_resize\n",
    "            \n",
    "            img_white = np.expand_dims(img_white / 255.0, axis=0)\n",
    "            predictions = model.predict(img_white)\n",
    "            predicted_class = np.argmax(predictions)\n",
    "            prediction = labels[predicted_class]\n",
    "        except Exception as e:\n",
    "            print(\"Prediction error:\", e)\n",
    "    \n",
    "    return prediction, bbox\n",
    "\n",
    "def predict_mlp(img):\n",
    "    \"\"\"Predict using MLP (Words)\"\"\"\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = holistic.process(img_rgb)\n",
    "    \n",
    "    landmarks = []\n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "    if results.left_hand_landmarks:\n",
    "        for landmark in results.left_hand_landmarks.landmark:\n",
    "            landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "    if results.right_hand_landmarks:\n",
    "        for landmark in results.right_hand_landmarks.landmark:\n",
    "            landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "    \n",
    "    prediction = \"\"\n",
    "    if len(landmarks) > 0:\n",
    "        landmarks = np.array(landmarks)\n",
    "        if len(landmarks) < EXPECTED_LANDMARK_SIZE:\n",
    "            landmarks = np.pad(landmarks, (0, EXPECTED_LANDMARK_SIZE - len(landmarks)))\n",
    "        elif len(landmarks) > EXPECTED_LANDMARK_SIZE:\n",
    "            landmarks = landmarks[:EXPECTED_LANDMARK_SIZE]\n",
    "        \n",
    "        predictions = model.predict(landmarks.reshape(1, -1))\n",
    "        predicted_class = np.argmax(predictions)\n",
    "        prediction = labels[predicted_class]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "def update_frame():\n",
    "    \"\"\"Update webcam feed and predictions\"\"\"\n",
    "    global recorded_gestures\n",
    "    \n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        return\n",
    "    \n",
    "    img_output = img.copy()\n",
    "    prediction = \"\"\n",
    "    mode_text = \"\"\n",
    "    \n",
    "    if current_model == \"cnn\":\n",
    "        prediction, bbox = predict_cnn(img)\n",
    "        mode_text = f\"Mode: {'Alphabets' if labels[0].isalpha() else 'Numbers'}\"\n",
    "        \n",
    "        # Draw bounding box and prediction if hand detected\n",
    "        if bbox:\n",
    "            x, y, w, h = bbox\n",
    "            cv2.rectangle(img_output, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img_output, prediction, (x, y - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "    elif current_model == \"mlp\":\n",
    "        prediction = predict_mlp(img)\n",
    "        mode_text = \"Mode: Words\"\n",
    "    \n",
    "    # Display mode and update text box\n",
    "    cv2.putText(img_output, mode_text, (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    if prediction:\n",
    "        text_box.delete(0, tk.END)\n",
    "        text_box.insert(0, prediction)\n",
    "    \n",
    "    # Display webcam feed\n",
    "    img_output = cv2.cvtColor(img_output, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_output)\n",
    "    img_tk = ImageTk.PhotoImage(image=img_pil)\n",
    "    canvas.create_image(0, 0, anchor=tk.NW, image=img_tk)\n",
    "    canvas.img_tk = img_tk\n",
    "    \n",
    "    root.after(10, update_frame)\n",
    "\n",
    "# ===== GUI Components =====\n",
    "# Control buttons\n",
    "tk.Button(button_frame, text=\"Record Gesture\", font=('Arial', 14), \n",
    "          command=lambda: recorded_gestures.append(text_box.get()) or update_recorded_text(), \n",
    "          bg=\"lightblue\").pack(side=tk.LEFT, padx=5)\n",
    "tk.Button(button_frame, text=\"Speak\", font=('Arial', 14), \n",
    "          command=lambda: engine.say(\" \".join(recorded_gestures)) or engine.runAndWait(), \n",
    "          bg=\"lightgreen\").pack(side=tk.LEFT, padx=5)\n",
    "tk.Button(button_frame, text=\"Clear\", font=('Arial', 14), \n",
    "          command=lambda: recorded_gestures.clear() or update_recorded_text(), \n",
    "          bg=\"salmon\").pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "# Model switch buttons\n",
    "tk.Button(mode_frame, text=\"Alphabets\", font=('Arial', 14), \n",
    "          command=lambda: load_model_cnn('alphabet_model.h5', list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")), \n",
    "          bg=\"lightyellow\").pack(side=tk.LEFT, padx=5)\n",
    "tk.Button(mode_frame, text=\"Numbers\", font=('Arial', 14), \n",
    "          command=lambda: load_model_cnn('number_model.h5', [str(i) for i in range(10)]), \n",
    "          bg=\"lightyellow\").pack(side=tk.LEFT, padx=5)\n",
    "tk.Button(mode_frame, text=\"Words\", font=('Arial', 14), \n",
    "          command=lambda: load_model_mlp('bothHands.h5',['Am','Done','Good','Help','I','Later','Morning','Night','Ok','Pray','See','Want','Water','When','You']), \n",
    "          bg=\"lightyellow\").pack(side=tk.LEFT, padx=5)\n",
    "\n",
    "def update_recorded_text():\n",
    "    recorded_text.config(text=\" \".join(recorded_gestures))\n",
    "\n",
    "# Initialize with Alphabets model by default\n",
    "load_model_mlp('bothHands.h5', ['Am','Done','Good','Help','I','Later','Morning','Night','Ok','Pray','See','Want','Water','When','You'])\n",
    "\n",
    "# Start the frame update loop\n",
    "update_frame()\n",
    "\n",
    "# Close handler\n",
    "def on_closing():\n",
    "    cap.release()\n",
    "    root.destroy()\n",
    "\n",
    "root.protocol(\"WM_DELETE_WINDOW\", on_closing)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b23ae-9f7d-4e1f-b065-f067639f4edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
